global:
  nameOverride: ""
  fullnameOverride: ""
  
  orchestyVersion: "2.1"
  frontend_url: https://pipes.example.com
  backend_url: https://backend.pipes.example.com
  starting_point_url: http://starting-point.pipes.example.com

  timezone: "Europe/London"

  useQuota: false
  namespaceQuota:
    resources:
      requests.cpu: 1000m
      requests.memory: 512Mi
      limits.cpu: 4000m
      limits.memory: 2Gi

  # defaults to internal worker-api service URL
  worker_api_url: ""

  orchestyImageRegistry:
    enablePullSecret: false
    server: docker.io
    path: orchesty
    username: ""
    password: ""

  imageOverrides: {}
    # note: you can currently override image used in topologies only by changing images.bridge

  images:
    applinth-marketplace-ui: ""
    backend: backend
    frontend: frontend
    cron-api: cron

    bridge: bridge
    counter: counter

    limiter: limiter
    starting-point: starting-point
    topology-api: topology-api
    fluentd: fluentd
    detector: detector
    worker-api: worker-api

  # Default global node selector
  nodeSelector: {}
  tolerations: []

  monitoring:
    logStorage: mongodb

    # Used only when logStorage is "google"
    # You must also add iam policy binding to that account:
    # gcloud iam service-accounts add-iam-policy-binding \
    #   --role roles/iam.workloadIdentityUser \
    #   --member "serviceAccount:PROJECT-ID.svc.id.goog[NAMESPACE/KSA-NAME]" \
    #   GSA-WITH-LOGS-WRITER-ROLE
    #
    # see https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#authenticating_to
    #
    googleServiceAccount: gsa-with-Logs-Writer-role

  api:
    serviceAnnotations: {}

  applinthMarketplaceUI:
    service:
      type: none
      port: 80
    auth_backlink: not-set
    branding_package_url: ""
    serviceAnnotations: {}
    resources:
      requests:
        memory: 8Mi
        cpu: 1m
      limits:
        memory: 12Mi
        cpu: 10m

  backend:
    service:
      type: none
      port: 80
    alpha_instance_id: not-set
    checkInfinityLoop: true
    resources:
      requests:
        memory: 80Mi
        cpu: 1m
      limits:
        memory: 96Mi
        cpu: 500m

  cronApi:
    resources:
      requests:
        memory: 20Mi
        cpu: 1m
      limits:
        memory: 32Mi
        cpu: 50m

  frontend:
    service:
      type: none
      port: 80
    serviceAnnotations: {}
    resources:
      requests:
        memory: 8Mi
        cpu: 1m
      limits:
        memory: 12Mi
        cpu: 10m

  limiter:
    resources:
      requests:
        memory: 20Mi
        cpu: 10m
      limits:
        memory: 32Mi
        cpu: 500m

  fluentd:
    resources:
      requests:
        memory: 100Mi
        cpu: 10m
      limits:
        memory: 128Mi
        cpu: 150m

  multiCounter:
    resources:
      requests:
        memory: 20Mi
        cpu: 5m
      limits:
        memory: 32Mi
        cpu: 500m

  startingPoint:
    service:
      type: none
      port: 8080
    serviceAnnotations: {}
    resources:
      requests:
        memory: 20Mi
        cpu: 10m
      limits:
        memory: 32Mi
        cpu: 500m

  detector:
    manageServiceAccount: true
    manageRBAC: true
    resources:
      requests:
        memory: 55Mi
        cpu: 10m
      limits:
        memory: 64Mi
        cpu: 100m

  topologyApi:
    manageServiceAccount: true
    manageRBAC: true
    resources:
      requests:
        memory: 50Mi
        cpu: 5m
      limits:
        memory: 64Mi
        cpu: 100m
    topologiesResources:
      defaultRequestCpu: 10m
      defaultRequestMemory: 50Mi
      defaultLimitCpu: 200m
      defaultLimitMemory: 128Mi
    topologiesExtraEnv: {}
#     FOO:
#       value: bar
    topologiesExtraSpec: {}
#      nodeSelector:
#        bridgepool: "true"
#      tolerations:
#        - effect: NoSchedule
#            key: bridgepool
#            operator: Equal
#            value: "true"

  worker:
    hostPathMounts: []
    nodeSelector: {}
    tolerations: []
    resources:
      requests:
        memory: 128Mi
        cpu: 50m
      limits:
        memory: 512Mi
        cpu: 1001m

  workers: {}
  # foo:
  #   sdk: nodejs
  #   image: NOT-PROVIDED
  #   extraEnv:
  #     bar:
  #       value: baz
  #   replicas: 1
  #   resources:
  #     requests:
  #       memory: 128Mi
  #       cpu: 50m
  #     limits:
  #       memory: 512Mi
  #       cpu: 1001m

  workerApi:
    serviceAnnotations: {}
    resources:
      requests:
        memory: 50Mi
        cpu: 5m
      limits:
        memory: 64Mi
        cpu: 250m
  
  logs:
    lokiHostname: loki-gateway.{{ .Release.Namespace }}.svc.cluster.local

    filter:

      # Namespaces to include. If empty, all namespaces are included (except excluded ones).
      # Example: ["default", "monitoring"]
      namespaces:
        include: []
        exclude: []

      # Pod name patterns to exclude (regex supported by River relabel)
      # Example: ["debug-.*", ".*-test"]
      pods:
        # Pod name patterns to include (regex). If empty, all pods are included (except excluded ones).
        include: []
        exclude: []

valkey:
  enabled: false # Enable it, if you want use Valkey(Redis) for cache

  replicaCount: 1
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi
  dataStorage:
    enabled: false # Enable it, if you want use persist storage
    requestedSize: 1Gi
    storageClass: local-path

# --- Grafana Configuration ---
grafana:
  enabled: false # Enable it, if you want use Grafana

  imageRenderer:
    enabled: false  # Because its colliding with image registry for Orchesty

  persistence:
    enabled: true
    size: 2Gi
    storageClassName: local-path
  
  # Use existing secret for admin credentials
  admin:
    existingSecret: pipes-secrets
    userKey: "admin-user"
    passwordKey: "admin-password"

  service:
    type: ClusterIP
    port: 80

  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
      - name: Loki
        type: loki
        url: "http://{{ .Values.global.logs.lokiHostname }}"
        access: proxy
        isDefault: true

# --- Loki Configuration ---
loki:
  enabled: false # Enable it, if you want use Loki

  deploymentMode: SingleBinary
  
  loki:
    auth_enabled: false
    
    commonConfig:
      replication_factor: 1
    
    storage:
      type: 'filesystem' # We use filesystem for chunks/rules locally, but object storage is configured below
    
    schemaConfig:
      configs:
      - from: 2024-01-01
        store: tsdb
        object_store: s3 # Use s3 defined in storage_config
        schema: v13
        index:
          prefix: index_
          period: 24h
    
    storage_config:
      aws:
        endpoint: "${S3_ENDPOINT}"
        bucketnames: "${S3_BUCKET}"
        access_key_id: "${S3_ACCESS_KEY}"
        secret_access_key: "${S3_SECRET_KEY}"
        s3forcepathstyle: true
    
    compactor:
      working_directory: /var/loki/compactor
      delete_request_store: s3
      retention_enabled: true
      retention_delete_delay: 2h
      retention_delete_worker_count: 150
    
    limits_config:
      retention_period: 720h # 30 days
      split_queries_by_interval: 5m
      max_line_size: 1MB
      ingestion_rate_mb: 10
      max_line_size_truncate: true
    
    server:
      grpc_server_max_send_msg_size: 33554432  # 32 MB
      grpc_server_max_recv_msg_size: 33554432  # 32 MB
    
  lokiCanary:
    enabled: false
  
  memberlist:
    service:
      publishNotReadyAddresses: true

  test:
    enabled: false

  chunksCache:
    allocatedMemory: 1024

  singleBinary:
    replicas: 1
    persistence:
      enabled: true
      size: 8Gi
      storageClass: local-path
    
    extraArgs:
      - '-config.expand-env=true'

    extraEnv:
        - name: S3_ENDPOINT
          valueFrom:
            secretKeyRef:
              name: pipes-secrets
              key: s3-endpoint
        - name: S3_BUCKET
          valueFrom:
            secretKeyRef:
              name: pipes-secrets
              key: s3-bucket
        - name: S3_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: pipes-secrets
              key: s3-access-key
        - name: S3_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: pipes-secrets
              key: s3-secret-key

  # Explicitly disable other components to avoid conflicts with SingleBinary mode
  read:
    replicas: 0
  write:
    replicas: 0
  backend:
    replicas: 0
  ingester:
    replicas: 0
  querier:
    replicas: 0
  queryFrontend:
    replicas: 0
  queryScheduler:
    replicas: 0
  distributor:
    replicas: 0
  compactor:
    replicas: 0
  indexGateway:
    replicas: 0
  bloomCompactor:
    replicas: 0
  bloomGateway:
    replicas: 0

# --- Alloy (Grafana Alloy) Configuration ---
alloy:
  enabled: false # Enable it, if you want use Alloy

  alloy:
    mounts:
      varlog: true
      extra:
        - name: varlib
          mountPath: /var/lib
          readOnly: true
    securityContext:
      privileged: true
      runAsUser: 0
      runAsGroup: 0
    
    configMap:
      create: false
      name: pipes-alloy

  controller:
    type: daemonset
    volumes:
      extra:
        - name: varlib
          hostPath:
            path: /var/lib
  
  # Disable internal metrics collection to keep it lightweight
  serviceMonitor:
    enabled: false